{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPbjoFxeEJ0dSode7CRfizw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swaroopkasaraneni/AI/blob/main/TextClassification1Case1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmKKDI2ngRjI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a program to input three sentences from user and creates the corpus\n",
        "\n",
        "Example: Let’s say these 3 are your strings:\n",
        "\n",
        "S1=” India won the match”\n",
        "\n",
        "S2=” England won the cricket match”\n",
        "\n",
        "S3=” Australia won the final match”Then Corpus (list of union of all words from all strings) is: [India, England, Australia, won, the, match, cricket, final]\n",
        "\n",
        "Create a function named “MakeCorpus” which will take list of string as an input and will return a list having union of all words. Save this function in a python file named “Corpus”. This can be used for future applications"
      ],
      "metadata": {
        "id": "vOKW3pqKgXXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Corpus.py\n",
        "\n",
        "# Function to create a corpus from a list of sentences\n",
        "def MakeCorpus(sentences):\n",
        "    \"\"\"\n",
        "    Takes a list of strings as input and returns a list with the union of all unique words.\n",
        "\n",
        "    Args:\n",
        "        sentences (list): List of strings (sentences).\n",
        "\n",
        "    Returns:\n",
        "        list: List containing unique words from all sentences.\n",
        "    \"\"\"\n",
        "    corpus = set()\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        corpus.update(words)\n",
        "    return list(corpus)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  S1 = input(\"Enter the first sentence: \")\n",
        "  S2 = input(\"Enter the second sentence: \")\n",
        "  S3 = input(\"Enter the third sentence: \")\n",
        "  sentences = [S1, S2, S3]\n",
        "  corpus = MakeCorpus(sentences)\n",
        "  print(\"\\nCorpus (list of unique words from all sentences):\")\n",
        "  print(corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P9Eh5Ojhxe3",
        "outputId": "ed318fb6-0ab2-45a4-e837-e93a4a669b3b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Corpus.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Corpus import MakeCorpus\n",
        "\n",
        "# Example usage\n",
        "sentences = [\"Hello world\", \"Hello AI\", \"Welcome to the future\"]\n",
        "corpus = MakeCorpus(sentences)\n",
        "print(corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0ekiTY6j3xt",
        "outputId": "7f959335-6431-408b-fc1d-984321c6b973"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['world', 'future', 'AI', 'Hello', 'to', 'the', 'Welcome']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a program to input three sentences from user and convert them into vectors.    Use presence and absence of words to build the vectors.Example: Let’s say these 3 are your strings:\n",
        "\n",
        "S1=” India won the match”\n",
        "\n",
        "S2=” England won the cricket match”\n",
        "\n",
        "S3=” Australia won the final match”Then Corpus (list of union of all words from all strings) is: [India, England, Australia, won, the, match, cricket, final]\n",
        "\n",
        "So,\n",
        "\n",
        "S1 will be  [1,0,0,1,1,1,0,0]\n",
        "\n",
        "S2 will be  [0,1,0,1,1,1,1,0]\n",
        "\n",
        "S3 will be  [0,0,1,1,1,1,0,1]\n",
        "\n",
        "Create a functionnamed “PresenceAbsenceVectorization” which will take list of string as an input and will return a list of vectors. Save this function ina python file named “Vectorization”. This can be used for future applications\n"
      ],
      "metadata": {
        "id": "mhKQumuDgzR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Vectorizations.py\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def TFIDFVectorization(sentences):\n",
        "    \"\"\"\n",
        "    Converts sentences into TF-IDF vectors using scikit-learn's TfidfVectorizer.\n",
        "\n",
        "    Parameters:\n",
        "    sentences (list of str): List of input sentences.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - feature_names (list of str): List of terms used as features.\n",
        "        - vectors (list of list of float): List of TF-IDF vectors for each sentence.\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    # Convert the TF-IDF matrix to a dense list of vectors\n",
        "    vectors = tfidf_matrix.toarray()\n",
        "    return feature_names, vectors\n",
        "\n",
        "def PresenceAbsenceVectorization(sentences):\n",
        "    \"\"\"\n",
        "    Converts sentences into presence/absence vectors based on unique words in the corpus.\n",
        "\n",
        "    Parameters:\n",
        "    sentences (list of str): List of input sentences.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - corpus (list of str): List of unique words from the input sentences.\n",
        "        - vectors (list of list of int): List of vectors corresponding to each sentence.\n",
        "    \"\"\"\n",
        "    corpus = set()\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        corpus.update(words)\n",
        "    corpus = sorted(corpus)\n",
        "\n",
        "    vectors = []\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        vector = [1 if word in words else 0 for word in corpus]\n",
        "        vectors.append(vector)\n",
        "    return corpus, vectors\n",
        "\n",
        "def CountVectorization(sentences):\n",
        "    \"\"\"\n",
        "    Converts sentences into count vectors based on unique words in the corpus.\n",
        "\n",
        "    Parameters:\n",
        "    sentences (list of str): List of input sentences.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - corpus (list of str): List of unique words from the input sentences.\n",
        "        - vectors (list of list of int): List of count vectors corresponding to each sentence.\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    corpus = set()\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        corpus.update(words)\n",
        "    corpus = sorted(corpus)\n",
        "\n",
        "    vectors = []\n",
        "    for sentence in sentences:\n",
        "        word_counts = Counter(sentence.split())\n",
        "        vector = [word_counts[word] for word in corpus]\n",
        "        vectors.append(vector)\n",
        "\n",
        "    return corpus, vectors\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW4V5qyplruf",
        "outputId": "a6f48cb5-d254-4300-f51b-0dcbbd2f74a0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Vectorizations.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import Vectorizations\n",
        "\n",
        "sentences = [\n",
        "    \"India won the match\",\n",
        "    \"England won the cricket match\",\n",
        "    \"Australia won the final match\"\n",
        "]\n",
        "\n",
        "corpus, vectors = Vectorizations.PresenceAbsenceVectorization(sentences)\n",
        "\n",
        "print(\"Corpus:\", corpus)\n",
        "for i, vector in enumerate(vectors, 1):\n",
        "    print(f\"Sentence {i} vector:\", vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwRotgKul8E_",
        "outputId": "2185cebb-055d-4e72-b3a6-24fbef05b12b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: ['Australia', 'England', 'India', 'cricket', 'final', 'match', 'the', 'won']\n",
            "Sentence 1 vector: [0, 0, 1, 0, 0, 1, 1, 1]\n",
            "Sentence 2 vector: [0, 1, 0, 1, 0, 1, 1, 1]\n",
            "Sentence 3 vector: [1, 0, 0, 0, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a program to enter 3 strings from a user and vectorise them on basis of their counts. Example: Let’s say these 3 are your strings:\n",
        "\n",
        "S1=” A lives with B.A plays with C. “\n",
        "\n",
        "S2=” B lives with C. B plays with D”\n",
        "\n",
        "S3=” C lives with D.C plays with A”Then Corpus (list of union of all words from all strings) is: [A, B, C, D,lives,with,plays]So, S1 will be  [2,1,1,0,1,2,1] S2 will be  [0,2,1,1,1,2,1]S3 will be  [1,0,2,1,1,2,1]\n",
        "\n",
        "Create a function named “CountVectorization” which will take list of string as an input and will return a list of vectors. Save this function in a python file named “Vectorization”. This can be used for future applications"
      ],
      "metadata": {
        "id": "QdKHL7SqhMuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import Vectorizations\n",
        "\n",
        "# Input sentences\n",
        "sentences = [\n",
        "    \"A lives with B. A plays with C.\",\n",
        "    \"B lives with C. B plays with D\",\n",
        "    \"C lives with D. C plays with A\"\n",
        "]\n",
        "\n",
        "# Generate count vectors\n",
        "corpus, vectors = Vectorizations.CountVectorization(sentences)\n",
        "\n",
        "# Display the results\n",
        "print(\"Corpus:\", corpus)\n",
        "for i, vector in enumerate(vectors, 1):\n",
        "    print(f\"Sentence {i} vector:\", vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra4afiLfmwMk",
        "outputId": "50ca67c4-14b1-4bce-b15a-7df3ca280a0f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: ['A', 'B', 'B.', 'C', 'C.', 'D', 'D.', 'lives', 'plays', 'with']\n",
            "Sentence 1 vector: [2, 0, 1, 0, 1, 0, 0, 1, 1, 2]\n",
            "Sentence 2 vector: [0, 2, 0, 0, 1, 1, 0, 1, 1, 2]\n",
            "Sentence 3 vector: [1, 0, 0, 2, 0, 0, 1, 1, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i72neAkwnzrd",
        "outputId": "9a0700f9-4b24-4ee1-8abc-646e46776933"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus.py  CVectorization.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  Vectorization.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a program to input 3 strings but vectorise them using TF-IDF (Term Frequency and Inverse Document Frequency) and print the strings along with the vectors.You can use already available python TF-IDF Vectorizer(Refer : http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)Create a function named “TFIDFVectorization” which will take listof string as an input and will return a list of vectors. Save this function in a python file named “Vectorization”. This can be used for future applications\n"
      ],
      "metadata": {
        "id": "4Bb4ZukUhVkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import Vectorizations\n",
        "\n",
        "# Input sentences\n",
        "sentences = [\n",
        "    \"India won the match\",\n",
        "    \"England won the cricket match\",\n",
        "    \"Australia won the final match\"\n",
        "]\n",
        "\n",
        "# Generate TF-IDF vectors\n",
        "feature_names, vectors = Vectorizations.TFIDFVectorization(sentences)\n",
        "\n",
        "# Display the results\n",
        "print(\"Feature Names (Corpus Terms):\", feature_names)\n",
        "for i, vector in enumerate(vectors, 1):\n",
        "    print(f\"Sentence {i} TF-IDF Vector:\", vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWxtTiN0hY9I",
        "outputId": "8c1a9a26-756d-4fb8-e93c-ce11b1790845"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names (Corpus Terms): ['australia' 'cricket' 'england' 'final' 'india' 'match' 'the' 'won']\n",
            "Sentence 1 TF-IDF Vector: [0.         0.         0.         0.         0.69903033 0.41285857\n",
            " 0.41285857 0.41285857]\n",
            "Sentence 2 TF-IDF Vector: [0.         0.57292883 0.57292883 0.         0.         0.338381\n",
            " 0.338381   0.338381  ]\n",
            "Sentence 3 TF-IDF Vector: [0.57292883 0.         0.         0.57292883 0.         0.338381\n",
            " 0.338381   0.338381  ]\n"
          ]
        }
      ]
    }
  ]
}